{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817bae51-17d6-49d6-a62c-204cf3d8c91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67540185-0d24-4c8a-938e-abc4e616306d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa511a6a-149f-458b-9ab0-3d7d7bd5935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cvzone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32148e1d-0606-4ade-ac82-829dc48d7fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvzone.HandTrackingModule import HandDetector\n",
    "from cvzone.ClassificationModule import Classifier\n",
    "import numpy as np\n",
    "import math\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49800b68-a05c-46fe-acab-7e52d1723cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "detector = HandDetector(maxHands=1)\n",
    "classifier = Classifier(\"C:/Users/20109/OneDrive/Desktop/Rudy/Sign-Language-detection/Model/keras_model.h5\" , \"Model/labels.txt\")\n",
    "offset = 20\n",
    "imgSize = 300\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "381fee43-e233-49a1-8951-937650504ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"Bed\",\"Child\",\"Cigarette\",\"Drink\",\"Family\",\"Food\",\"Forget\",\"Hello\",\"I Love You\",\"Know\",\"No\", \"Please\",\"Sorry\",\"Thank You\", \"Thirsty\", \"Yes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c343571-d2e4-4eca-92f5-ee80500ebbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 41ms/step\n",
      "[6.7374465e-05, 2.251511e-06, 0.039752908, 1.8588666e-05, 0.02176797, 0.122518755, 9.410881e-05, 1.234044e-06, 0.8114993, 9.239213e-07, 6.323112e-08, 1.7579938e-05, 1.185916e-05, 0.00063172297, 0.003202091, 0.00041316578] 8\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "[0.00034610557, 6.322321e-07, 0.04621419, 1.058627e-06, 0.0020797749, 0.0068486673, 1.5189106e-05, 4.0614403e-05, 0.94145566, 9.203232e-07, 3.038589e-06, 5.1098014e-05, 7.639117e-07, 0.0028514322, 8.931766e-05, 1.5667068e-06] 8\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "[3.1384247e-05, 2.6821567e-07, 4.0592553e-05, 4.6777627e-06, 0.00021533422, 0.0016369771, 2.405326e-06, 5.0449927e-07, 0.9979413, 1.0731886e-07, 9.961354e-07, 8.274466e-06, 1.1724411e-06, 1.4647712e-05, 0.00010043764, 9.1568626e-07] 8\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "[6.6907924e-06, 5.5552715e-07, 2.2128808e-05, 1.8423314e-06, 3.993259e-06, 4.7910517e-06, 4.3557208e-08, 4.371702e-06, 0.9998374, 5.360585e-07, 6.661171e-07, 1.2435132e-06, 8.948139e-08, 0.00010860089, 6.6096018e-06, 3.187396e-07] 8\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "[1.4236504e-06, 1.9933864e-08, 1.269368e-06, 8.8937804e-07, 7.8356084e-07, 6.9238454e-06, 1.244803e-08, 1.0670974e-07, 0.9999796, 4.393582e-08, 4.6062814e-07, 7.2330965e-07, 2.0453197e-07, 4.247905e-06, 3.1882644e-06, 3.7067668e-08] 8\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "[1.6470352e-05, 3.134147e-07, 4.9650713e-05, 1.3958911e-06, 7.3836854e-06, 0.00040620368, 2.1606463e-06, 1.2123063e-06, 0.99941826, 7.143476e-08, 7.958202e-08, 5.6107257e-07, 4.5454115e-07, 8.9462956e-05, 4.3927116e-06, 1.9962938e-06] 8\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "[0.00011181271, 5.0178023e-06, 0.00014286737, 1.0298533e-05, 3.332775e-05, 0.001884846, 6.488047e-06, 2.0175426e-06, 0.9974131, 2.6112352e-06, 3.947479e-06, 3.7292161e-06, 7.4328377e-06, 0.00034538232, 1.8111241e-05, 8.937465e-06] 8\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "[0.00038215338, 4.844456e-05, 0.015110989, 0.00020378207, 0.0039141537, 0.23166591, 0.00011784142, 8.370397e-05, 0.7390458, 3.1008283e-06, 3.435807e-05, 0.00011042046, 0.0003629079, 0.0037289641, 0.0045467243, 0.00064073456] 8\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    success, img = cap.read()\n",
    "    imgOutput = img.copy()\n",
    "    hands, img = detector.findHands(img)\n",
    "    if hands:\n",
    "        hand = hands[0]\n",
    "        x, y, w, h = hand['bbox']\n",
    "\n",
    "        imgWhite = np.ones((imgSize, imgSize, 3), np.uint8)*255\n",
    "\n",
    "        imgCrop = img[y-offset:y + h + offset, x-offset:x + w + offset]\n",
    "        imgCropShape = imgCrop.shape\n",
    "\n",
    "        aspectRatio = h / w\n",
    "\n",
    "        if aspectRatio > 1:\n",
    "            k = imgSize / h\n",
    "            wCal = math.ceil(k * w)\n",
    "            imgResize = cv2.resize(imgCrop, (wCal, imgSize))\n",
    "            imgResizeShape = imgResize.shape\n",
    "            wGap = math.ceil((imgSize-wCal)/2)\n",
    "            imgWhite[:, wGap: wCal + wGap] = imgResize\n",
    "            prediction , index = classifier.getPrediction(imgWhite, draw= False)\n",
    "            print(prediction, index)\n",
    "\n",
    "        else:\n",
    "            k = imgSize / w\n",
    "            hCal = math.ceil(k * h)\n",
    "            imgResize = cv2.resize(imgCrop, (imgSize, hCal))\n",
    "            imgResizeShape = imgResize.shape\n",
    "            hGap = math.ceil((imgSize - hCal) / 2)\n",
    "            imgWhite[hGap: hCal + hGap, :] = imgResize\n",
    "            prediction , index = classifier.getPrediction(imgWhite, draw= False)\n",
    "\n",
    "       \n",
    "        cv2.rectangle(imgOutput,(x-offset,y-offset-70),(x -offset+400, y - offset+60-50),(0,255,0),cv2.FILLED)  \n",
    "\n",
    "        cv2.putText(imgOutput,labels[index],(x,y-30),cv2.FONT_HERSHEY_COMPLEX,2,(0,0,0),2) \n",
    "        cv2.rectangle(imgOutput,(x-offset,y-offset),(x + w + offset, y+h + offset),(0,255,0),4)  \n",
    "\n",
    "    cv2.imshow('Image', imgOutput)\n",
    "    if (cv2.waitKey(1) & 0xFF) ==ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11992b9a-04ac-4d36-b8b4-35a36a5b003b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
